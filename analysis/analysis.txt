Victor Chu
vic4

Run WordGramDiver for wordgrams of size 2-10 and record
the number of WordGram values/objects that occur more than
once as reported by the runs. For example, with WSIZE = 2,
which generates 2-grams, the output of benchmark and benchmarkShift
each indicates that the total # wordgrams generated is 177,634
and that the # unique wordgrams is 117,181

This means there are 177,634 - 117,181 = 60,453 WordGram values that
occur more than once. Find these same values for other orders
of k and complete the table below for different k-grams/different 
values of WSIZE

WSIZE    # duplicates //do wgs - size. 
2        60,453
3        10,756
4		 1,987
5		 667
6		 362
7		 226
8	     151
9	     105
10    	 73

=====
Explain in your own words the conceptual differences between 
the benchmark and benchmarkShift methods. 

Each method shifts the words around, to return the total
unique words. In each, you add words continuously, but 
you still get the same amount of unique words due to 
them being stored in sets. Conceptually, the first method
adds our words, to a list, then creates a wordgram out of them. 
The second one creates words grams directly, adding each 
wordgram directly to a set. We still get the same amount of unique letters
The second method, benchmarkShift runs quicker, because
we aren't creating wordgrams again and again like we are in the first one
which takes looping through the words again and again. 

Answer these questions: 


(1) Why the results of these methods should be the same in 
terms of changes made to the HashSet parameter.

They both shift the words around, but they do it different ways. You add 
 words continuously to the wordgram, but in different ways. You add both 
to a hashset so there are not any duplicates in either one. Wordgram sets have
the same amount of entries because of this hashset. Because of no duplicates,
you get the same amount of words in each. 


(2) What are the conceptual differences between the two
benchmarking methods

First one, benchmark, adds our words into a string arraylist, later
converted to an array. Then we create a wordgram out of each 
successive word in our array, adding it to a set so
we avoid duplicates. We return how many unique wordgrams we have. 
In the second benchmark method, benchmarkShift, we
still create a list of words, but we add one wordgram
directly to a set.Then, every wordgram after that
as well is added to the set. We still get the same amount of unique wordgrams.

(3) Is the total amount of memory allocated for arrays
the same or different in the two methods? Account for
arrays created in the methods and arrays created by
WordGram objects. Try to be quantitative in answering.

The amount of memory required is more in the first method. 
One of the reasons this is due to, is because you are constantly creating
a Wordgram, which require looping through the words again and again. 
The quantitative way to figure this out, is to compare the time 
it takes to run, in which the benchmark time takes
consistently longer. 
